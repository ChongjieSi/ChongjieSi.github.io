---
title: 矩阵的秩、矩阵的子空间、SVD分解
summary: 一些我对矩阵的秩、子空间以及SVD分解的理解
date: 2024-10-11
type: docs
math: true
tags:
  - 数学漫谈
image:
  caption: 
---


<!-- {{< math >}} $$ {{< /math >}} -->

写一点我对矩阵的秩、矩阵的子空间以及SVD分解的理解。

## 矩阵的秩

### 一个简单的例子

我对矩阵秩的最初直观认识源于在多标签学习中的实践。在多标签学习中，一个关键的挑战在于标签之间并非独立，而是存在一定的关联性。以一个简单的例子说明：若某个样本同时具有“雨林”和“足球”两个标签，我们自然会推测它是否也会有“巴西”标签。这表明标签之间存在某种关联结构，而非完全独立。为了进一步说明，我们假设有三个样本，并将它们的标签用一个矩阵来表示，其中矩阵的列依次代表“雨林”、“足球”和“巴西”，每一行则对应一个样本的标签：{{< math >}} $\left[\begin{matrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 1 & 1 \end{matrix}\right]$ {{< /math >}}。不难发现，矩阵的秩为 2。这反映了样本的标签之间存在一定的关联性或冗余性，换句话说，并不是每个标签都独立于其他标签。具体到多标签学习中，矩阵的低秩说明标签之间的相关性较强，部分标签可以通过其他标签的组合来近似表示。对于这个矩阵而言，尽管它包含三个标签，但实际上只有两个潜在的“信息源”或“特征维度”，这解释了为什么秩为 2。

### 秩的含义与作用

从数学的角度来看，矩阵的秩（rank）是指矩阵中独立行或独立列的最大数目。换句话说，秩衡量的是矩阵中能产生线性独立向量的数量。对于一个{{< math >}} $ m \times n$ {{< /math >}}的矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}，秩{{< math >}} $r$ {{< /math >}} 表示矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}中有{{< math >}} $r$ {{< /math >}}个线性独立的列或行，其余的列或行可以通过这些线性独立向量的线性组合表示。

矩阵秩在多个领域中扮演重要角色，尤其在数据分析和机器学习中，它具有以下几个关键作用：

1. 线性独立性：秩还反映了矩阵中数据的线性独立性。高秩矩阵表示大部分行或列之间是线性独立的，信息量较高。低秩矩阵表示数据中存在线性相关性，部分信息可以由其他信息推导出。在线性代数中，这一性质可以用于解线性方程组、评估系统的自由度等。
2. 冗余信息的识别：当矩阵的秩小于其行或列的数量时，这表明矩阵中存在冗余信息。例如，在多标签学习中，如果标签之间高度相关，标签矩阵的秩会比标签的总数小。通过分析矩阵的秩，可以发现并去除这些冗余信息，从而提高学习算法的效率。
3. 信号与噪声的分离：在信号处理和机器学习中，秩常用于将信号与噪声分离出来。例如，通过低秩近似，可以滤除噪声并保留数据的核心信息。

用一句话概括，秩衡量了矩阵行或者列之间的线性无关性。

在理解了矩阵的秩之后，我们可以进一步探讨与秩密切相关的概念：矩阵的子空间。矩阵的秩不仅揭示了其行或列之间的线性独立性，还引导我们理解矩阵所作用的空间结构。为了更好地理解这一点，我们引入矩阵的子空间这一概念。

## 矩阵的子空间

### 定义

在线性代数中，子空间是一个由向量构成的线性空间，它封闭于向量加法和数乘运算。对矩阵而言，主要关注的是列空间（column space）、行空间（row space）和零空间（null space）。这些子空间是理解矩阵映射几何特性的核心要素，能够帮助我们揭示矩阵如何将向量从一个空间映射到另一个空间。

### 列空间（Column Space）

列空间是矩阵的所有列向量的线性组合所张成的向量空间。给定一个{{< math >}} $ m \times n$ {{< /math >}}的矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}，其列空间是{{< math >}} $  \mathbb{R}^m  $ {{< /math >}} 中的一个子空间。列空间中的任何向量都可以由矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的列向量通过线性组合得到。列空间的维度称为秩，表示该矩阵能够张成的线性独立方向的数量。

例如，如果矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的列向量中有线性相关的列，则这些相关的列不贡献新的独立方向，从而列空间的维度会小于矩阵的列数。因此，矩阵的秩实际上等于列空间的维度。

几何上，列空间描述了矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}如何将输入向量（通常来自{{< math >}} $  \mathbb{R}^m  $ {{< /math >}} ）映射到其输出空间（通常是{{< math >}} $  \mathbb{R}^m  $ {{< /math >}} 中的一个子空间）。列空间张成的维度越小，说明矩阵能够覆盖的方向越少，矩阵的映射效果越“压缩”。

### 行空间（Row Space）

行空间是由矩阵的所有行向量的线性组合所构成的向量空间。行空间与列空间密切相关，但存在于不同的维度中：行空间位于{{< math >}} $  \mathbb{R}^n $ {{< /math >}}，而列空间位于{{< math >}} $  \mathbb{R}^m  $ {{< /math >}}。行空间的维度同样为矩阵的秩，即矩阵的行中有多少是线性独立的。

行空间可以看作是矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的“左零空间”的正交补。几何上，行空间描述了矩阵的输入空间中所有可以由行向量表示的方向。换句话说，矩阵的行空间反映了哪些方向的输入向量会对输出产生影响，其他方向则被“映射到零”。

### 零空间（Null Space）

零空间，也称为核（kernel），是矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}映射到零向量的所有输入向量的集合。零空间的维度称为零空间维度（nullity），它反映了矩阵映射中失去的信息量。换句话说，零空间的维度告诉我们有多少个输入方向被矩阵映射到零。

零空间不仅仅是一个抽象的概念，它在解线性方程组时扮演着重要角色。假设我们解方程{{< math >}} $\mathbf{A} \mathbf{x} = \mathbf{0}$ {{< /math >}}，零空间正是所有满足此方程的向量 {{< math >}} $x$ {{< /math >}}的集合。零空间的维度与秩相关，由秩-零空间定理（Rank-Nullity Theorem）给出：

{{< math >}} $$\text{rank}(\mathbf{A}) + \text{nullity}(\mathbf{A}) = n$$ {{< /math >}}

其中{{< math >}} $n$ {{< /math >}}是矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的列数。这个定理表明，矩阵的零空间维度与矩阵的秩共同决定了输入空间的整体结构。
几何上，零空间对应于矩阵无法区分的输入方向。这意味着，输入空间中的一些方向对输出空间没有贡献，它们被映射到零。因此，零空间的维度越大，矩阵的信息损失就越严重。

### 左零空间（Left Null Space）

与零空间相对的，还有左零空间。左零空间是矩阵{{< math >}} $\mathbf{A}^\mathsf{T}$ {{< /math >}}的零空间，即所有满足{{< math >}} $\mathbf{A}^T \mathbf{x} = \mathbf{0} $ {{< /math >}}的向量构成的子空间。几何上，左零空间表示的是输入向量对行空间的正交部分。在矩阵分解中，左零空间与行空间的正交性起着重要作用。它帮助我们理解哪些输入方向与矩阵的行没有重叠，从而被完全“忽略”。

### 子空间与秩的关系

一个非常重要的事实是：矩阵的行空间和零空间是正交的，矩阵的列空间和左零空间也是正交的。这种正交性提供了一种简化分析矩阵映射的方式。如果一个向量位于矩阵的零空间，它与行空间中的所有向量正交，这意味着它不会对矩阵的输出产生贡献。同理，左零空间中的向量不会影响矩阵的输入行为。

秩实际上是列空间和行空间维度的共同度量。换句话说，秩代表了矩阵可以在其列空间和行空间中张成的最大独立方向数。因此，理解子空间之间的正交性和秩之间的关系，是理解矩阵如何作用于向量空间的关键。

## 奇异值分解

在之前的讨论中，我们介绍了矩阵的子空间，如列空间、行空间和零空间，及其在理解矩阵结构和行为中的作用。然而，子空间分析并不足以揭示矩阵的全部复杂性。为此，我们引入奇异值分解（Singular Value Decomposition, SVD）这一工具。SVD 不仅揭示矩阵的秩和子空间，还通过一系列几何操作（旋转和缩放）解析矩阵的深层结构。SVD 分解能够紧凑地表示矩阵的主要特性，使其作用过程更加直观。接下来，我们将深入探讨 SVD 的具体细节及其几何意义。

### 奇异值分解的复杂数学背景

给定一个任意的{{< math >}} $ m \times n$ {{< /math >}}的矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}，我们可以通过 SVD 分解将其表示为：

{{< math >}} $$ \mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\mathsf{T} $$ {{< /math >}}

其中：

- {{< math >}} $\mathbf{U}$ {{< /math >}}是一个{{< math >}} $ m \times m$ {{< /math >}}的正交矩阵，表示矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的左奇异向量，这些向量构成了{{< math >}} $\mathbf{A}$ {{< /math >}}的列空间的正交基；
- {{< math >}} $ \mathbf{\Sigma} $ {{< /math >}}是一个{{< math >}} $ m \times n$ {{< /math >}}的对角矩阵，其中的对角元素是矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的奇异值，表示矩阵在不同方向上的伸缩比例；
- {{< math >}} $\mathbf{V}$ {{< /math >}}是一个{{< math >}} $ n \times n$ {{< /math >}}的正交矩阵，表示矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的右奇异向量，它们构成了{{< math >}} $\mathbf{A}$ {{< /math >}}的行空间的正交基。

SVD 分解展示了矩阵如何在三个步骤中作用于输入向量：

1. 旋转输入向量到右奇异向量基下的空间（通过{{< math >}} $\mathbf{V}^\mathsf{T}$ {{< /math >}}）；
2. 沿着每个奇异值的方向进行缩放（通过{{< math >}} $\mathbf{\Sigma}$ {{< /math >}}）；
3. 将缩放后的向量再次旋转到左奇异向量基下的空间（通过{{< math >}} $\mathbf{U}$ {{< /math >}}）。

### 奇异值：矩阵几何行为的核心

奇异值是 SVD 分解中的核心元素，它们揭示了矩阵在不同方向上的缩放强度。对于一个矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}，奇异值{{< math >}} $\sigma_i$ {{< /math >}}是矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的拉伸或压缩比例，且总是非负的。通过奇异值的排列，矩阵可以分解为一系列基于强度递减的作用方向。

奇异值的数量等于矩阵的秩，因此，非零奇异值的个数决定了矩阵能够张成的线性独立方向的数量。奇异值为零意味着矩阵在某些方向上完全压缩，这些方向正是矩阵的零空间。换句话说，零奇异值描述了哪些方向在映射过程中被“丢失”或映射到零。

几何上，奇异值控制了矩阵将单位球体拉伸为一个椭球体的形状。每个奇异值对应椭球体的一个半轴的长度，非零奇异值的数量决定了椭球体的维数。此时，我们可以将矩阵看作是一个在高维空间中的变换，它通过奇异值来描述不同维度的拉伸和压缩。

### 左奇异向量与右奇异向量：矩阵映射的几何旋转

SVD 中的{{< math >}} $\mathbf{U}$ {{< /math >}}和{{< math >}} $\mathbf{V}$ {{< /math >}}分别是矩阵{{< math >}} $\mathbf{A}$ {{< /math >}}的左奇异向量和右奇异向量，它们构成了矩阵作用空间的正交基。在几何上，右奇异向量{{< math >}} $\mathbf{V}$ {{< /math >}}是矩阵的输入空间的特征方向，它们描述了矩阵在这些方向上对输入向量的作用。左奇异向量{{< math >}} $\mathbf{U}$ {{< /math >}}是输出空间的特征方向，表示矩阵在输出空间中将输入向量映射到的结果方向。

右奇异向量可以理解为矩阵在输入空间中的“首选”方向，矩阵在这些方向上表现得最为明显。通过{{< math >}} $\mathbf{V}^\mathsf{T}$ {{< /math >}}，矩阵将任意输入向量旋转到这些特定的方向上。左奇异向量则是输出空间的“首选”方向，通过{{< math >}} $\mathbf{U}$ {{< /math >}}，矩阵将缩放后的向量旋转到输出空间的这些特定方向上。

这两个正交基的存在确保了 SVD 分解在高维空间中依然能够精确地描述矩阵的行为。尤其是在维度压缩、特征提取等问题中，右奇异向量对应于数据的主方向，而左奇异向量则对应于数据在新空间中的主成分。
