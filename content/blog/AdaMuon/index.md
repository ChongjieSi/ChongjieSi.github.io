---
title: AdaMuon：能否进一步助力Muon替代Adam？
summary: 优化器
date: 2025-08-27
type: docs
math: true
tags:
  - 论文相关
  - 优化器
image:
  caption: 
---


<!-- {{< math >}} $$ {{< /math >}} -->

在上一篇文章中，我们介绍了Muon以及其相关工作，并验证了Muon对比Adam的优势。

在这一篇文章中，我打算进一步介绍我们在Muon上的探索，以及我们的新方法AdaMuon。所有的代码已经开源。

多啰嗦一句，我觉得nanoGPT这个代码框是真的好用，改一改模型或者优化器，很快就能出结果了，很适合作为学术界发论文或者工业界的一个小规模验证实验。另外Megatron-LM 的代码也一并附上了。

如果对Muon或者Adam还不太了解的读者，可以参考上一篇文章，这篇将在上一篇的基础上进行。 (想起来一个经典笑话：Who is Adam?)

## 闲聊一下

今年3月份，我在X上闲逛，偶然间看到一个叫做 Muon 的优化器。当时看到评论区的人有的是一顿狠吹，说是训练效率大幅提升，并会取代Adam；也有的人唱衰，说不好用，没有任何效果。其实直到今天关于Muon也大致分为这两派人，当然这是后话。

我本着吃瓜的态度，搜索了一下Muon的论文，结果 google scholar 上没有，arxiv 上也没有，最后才发现是一篇博客。当看完了博客，我的第一反应是质疑，因为 Muon 和 Shampoo 实在是太像了，甚至可以说是 Shampoo 的简化版：

<img src="https://pic1.zhimg.com/v2-cd56762ee0afcfc9c2a229044fc1dd30_1440w.jpg">

Muon 与 Shampoo 的核心区别在于是否对预条件矩阵（preconditioner）进行历史累计——Shampoo 通过不断累加历史信息（虽不同于典型的 EMA，但本质仍是累积），而 Muon 则没有这一机制。从动量优化的角度来看，我认为 Shampoo 应当具备更好的效果。之前也听说 Gemini 在用 Shampoo 训练，这也在一定程度上支持了我的判断。

不过，Shampoo 需要计算矩阵逆，计算开销较大；而 Muon 借助牛顿迭代法避免了显式求逆，在一定程度上可能比 Shampoo 更高效。但真正引起我注意的是苏剑林老师团队近期的工作。虽然我没有找到 Muon 的原论文，但发现了一篇刚刚发布的研究：

[Muon is scalable for LLM training](https://arxiv.org/abs/2502.16982)

这一发现让我颇感意外，也隐约看到了新的可能性。既然工业界已经开始关注 Muon，并对其扩展性进行了实证分析，也许Muon真的有望成为Adam的替代方案。不过，当时除了 Kimi 之外，我并未看到更多论文对 Muon 展开深入探索，一切尚处于未知阶段。这也让我有些犹豫：转向一个新方向必然带来一定的成本，而当时我正专注于 model merging 方面的研究，同时 hold 两个方向虽然可行，但前者已是一片红海，后者却仍迷雾重重——鸡蛋固然不应放在一个篮子里，但资源的分配也需谨慎权衡。

但是，在经过短暂的考虑之后，我当下决定切换方向——之前的不做了。学界需要追求新的东西，业界需要的是真正有用的东西。既然现在还没有多少人关注，或许只是因为大家尚在观望。如果日后它真的展现出巨大价值，即便我吃不上肉，说不定也能跟着喝上一口汤。

我到现在还记得，当我告诉导师我打算做优化器时，他脸上那种震惊和不解的神情。也许他也没想明白，为什么在 LLM 如此火热的当下，我会突然对一个小众的领域产生兴趣。

不过谁又能想到，今天 Kimi v2 1T 的模型用Muon训练成功了，GLM4.5 也用Muon训练成功了呢？

## 对 Muon 的分析

好了，闲话不多说。开启一个新方向，第一要务就是打通一套可靠的 baseline。我选择了一个基于 nanoGPT 修改而来的代码仓库作为起点。在实现了 Muon 优化器之后，初步的实验结果却并不理想。Muon 在训练早期往往比 Adam 收敛得更快，但在中后期却容易被反超。对于一个新兴方法来说，到这个阶段很容易让人怀疑其有效性。但出于一种直觉和信念，我总觉得应该是代码的某个地方出了问题。

经过逐行排查（幸亏代码量不大），我意外地发现了 Hugging Face 在 GPT-2 的注意力机制中的一个设置：

```python
if self.scale_attn_by_inverse_layer_idx:
    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)) / float(self.idx_layer + 1))
else:
    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
```

简单来说，原实现中对注意力输出额外除以了层数（layer），使得注意力值被人为缩小。这一做法相当奇怪——我从未在论文中见过类似描述，也没有看到任何理论依据，但它却实实在在地出现在了代码中。

在我将这个hack掉之后，训练结果立刻回归正常。Muon 的表现显著提升，相比 Adam 取得了 20% 到甚至 50% 的训练效率增益。通俗来讲，在最优设置下，Muon 仅需训练 10B token 所达到的验证损失，就相当于 Adam 训练 20B token 的结果（所有的实验结果图我会放到最后）。紧接着Megatron-LM上的Muon也复现成功，在大规模实验上Muon也展示出了惊人的效率。

这个过程看似轻松，实际上耗费了我近两个月的时间。此外，它也给了我一个重要的启示：某些模型架构可能对 Muon 并不友好。由于现有的模型大多基于 Adam 优化器进行训练和调优，许多被广泛采用的设计实际上是在 Adam 的语境下被验证有效的——我们可以认为这些架构是“为 Adam 优化”的。这一发现也为未来的模型设计提供了新的思路：在探索新的优化方法时，我们或许也需要重新审视和调整模型结构本身，以更好地适配优化器的特性，而不仅仅默认沿用为 Adam 所设计的架构。

既然 Muon 已经验证有效，接下来的目标自然是探索能否构建一个更高效的改进版本。从整体框架来看，Muon 可被视为一种二阶优化器，因为它利用了参数矩阵的行和列信息来构造预条件矩阵。不过，我更倾向于从矩阵操作的角度来理解：如果说 SGD 是元素级别的一阶优化器，那么 Muon 实际上是一种矩阵级别的一阶优化器——它的核心在于对一阶动量进行矩阵层级的更新。

于是，一个自然的思路是：能否为 Muon 引入矩阵层级的二阶动量，从而将这类 SGD 风格的优化器提升到 Adam 的级别？这一步目前看来仍有困难。既然如此，我们不妨先退一步思考：能否先为其引入元素级别的二阶动量，以增强其自适应能力？元素级二阶动量的重要性已无需赘言，其有效性早已在大量实验和实践中得到广泛验证。

这就是AdaMuon的motivation。

## AdaMuon

### 难点

要想实现一个二阶动量加持的Muon，最直接的想法是像Adam那样对Muon进行扩展，直接引入一个二阶动量累积项，并利用该方差估计对正交更新矩阵  进行缩放。然而，这种做法引入了两个关键的设计挑战：

1. 首先，应确定对哪个分量进行累积。在 Adam 中，二阶动量自然是在原始梯度 {{< math >}} $\mathbf{G}$ {{< /math >}} 上累积的；但在 Muon 中，更新方向 {{< math >}} $\mathbf{O}$ {{< /math >}} 是通过动量 {{< math >}} $\mathbf{M}$ {{< /math >}} 的极分解得到的。目前尚不清楚方差跟踪应当应用于 {{< math >}} $\mathbf{G}$ {{< /math >}}、 {{< math >}} $\mathbf{O}$ {{< /math >}} 还是 {{< math >}} $\mathbf{M}$ {{< /math >}} ，因为每一种选择都会导致不同的稳定性和归一化行为。
2. 其次，如何实现与 Adam 相当的归一化效果。在 Adam 中，一阶矩和二阶矩均基于同一梯度信号计算，因此它们的比值可有效实现逐元素的步长归一化。然而在 Muon 中， {{< math >}} $\mathbf{M}$ {{< /math >}} 在训练早期和中期可能波动剧烈，导致生成的 {{< math >}} $\mathbf{O}$ {{< /math >}} 在不同坐标间差异显著。因此，无论二阶动量是在{{< math >}} $\mathbf{G}$ {{< /math >}}、{{< math >}} $\mathbf{O}$ {{< /math >}}  还是  {{< math >}} $\mathbf{M}$ {{< /math >}} 上积，所得的统计量仍不稳定，无法提供可靠的归一化效果，甚至可能放大噪声。

要顺利将方差自适应能力融入 Muon，同时不破坏其正交化优势和固有的尺度不变性，就必须审慎解决上述挑战。

### 方法

针对第一个挑战，我们选择在正交矩阵 {{< math >}} $\mathbf{O}$ {{< /math >}} 上累积二阶动量，而非原始梯度 {{< math >}} $\mathbf{G}$ {{< /math >}} 或动量缓冲 {{< math >}} $\mathbf{M}$ {{< /math >}} 。该设计基于两点考虑：首先，原始梯度 {{< math >}} $\mathbf{G}$ {{< /math >}} 带有病态缩放和方向噪声，而 Muon 的极分解正是为消除此类噪声而设计，因此不适合用于稳定的方差估计；其次，虽然 {{< math >}} $\mathbf{M}$ {{< /math >}} 提供了时间平滑的梯度，但其元素值在训练早中期仍不稳定。此外，由于实际更新方向来自 {{< math >}} $\mathbf{O}$ {{< /math >}} ，在 {{< math >}} $\mathbf{M}$ {{< /math >}} 上累积方差会导致与更新所用的缩放基准不匹配。相比之下， {{< math >}} $\mathbf{O}$ {{< /math >}} 提供了几何归一化且稳定的下降方向，为方差估计提供了更干净的基准。

具体来说，我们像Adam一样，对 {{< math >}} $\mathbf{O}$ {{< /math >}} 逐元素平方进行指数移动平均：

{{< math >}} $$\mathbf{V}_t = \beta \cdot \mathbf{V}_{t-1} + (1 - \beta) \cdot (\mathbf{O}_t \odot \mathbf{O}_t), $$ {{< /math >}}

其中 {{< math >}} $\odot$ {{< /math >}} 表示哈达玛积。{{< math >}} $\mathbf{V}$ {{< /math >}} 作为正交更新的二阶动量缓冲区，功能类似于 Adam 中的方差累积器。系数 {{< math >}} $\beta$ {{< /math >}}  直接沿用 Muon 的动量参数，这样也可以不引入额外超参数。方差归一化后的更新方向为：

{{< math >}} $$\widehat{\mathbf{O}}_t = {\mathbf{O}_t} \oslash ({\sqrt{\mathbf{V}_t} + \varepsilon}), $$ {{< /math >}}

其中 {{< math >}} $\oslash$ {{< /math >}} 表示逐元素除法。之后我们也可以像Muon那样，对齐 Adam 更新量的 RMSnorm ：

{{< math >}} $$\gamma_t = \frac{0.2}{{\rm RMS}(\widehat{\mathbf{O}}_t)} = \frac{0.2\sqrt{mn}}{\|\widehat{\mathbf{O}}_t\|_F}, \ \ \mathbf{W}_{t+1} = \mathbf{W}_t - \eta \big(\gamma_t \widehat{\mathbf{O}}_t + \lambda \mathbf{W}_t\big). $$ {{< /math >}}

看上去开始像模像样了，但这么做的效果并不算是多好。主要原因是，动量缓冲 {{< math >}} $\mathbf{M}$ {{< /math >}} 在元素层面仍会剧烈变化，导致极分解产生的 {{< math >}} $\mathbf{O}$ {{< /math >}} 在不同坐标间波动显著。直接累积不稳定的 {{< math >}} $\mathbf{O}$ {{< /math >}} 难以实现有意义的归一化，甚至可能放大噪声。

为使 {{< math >}} $\mathbf{O}$ {{< /math >}} 既可用于二阶动量缩放又足够稳定，我们提出先对 {{< math >}} $\mathbf{O}$ {{< /math >}} 施加变换 {{< math >}} $f$ {{< /math >}} ，再执行极分解  {{< math >}} $g$ {{< /math >}}

{{< math >}} $$\mathbf{O}=g(f(\mathbf{M}))$$ {{< /math >}}

注意到 {{< math >}} $g$ {{< /math >}} 具有全局尺度不变性，即 {{< math >}} $g(c\mathbf{M}) = g(\mathbf{M}), \ c>0 $ {{< /math >}} ，但其本身无法稳定元素级波动。因此，{{< math >}} $f$ {{< /math >}}  需在保持坐标方向性的同时降低波动。最终我们选择 {{< math >}} $f={\rm sign}(\cdot)$ {{< /math >}} ，理由如下：

<img src="https://pic1.zhimg.com/v2-b85630809d7b788d334e95c38022d96c_1440w.jpg">

设计这些条件的原因在于

<img src="https://pic3.zhimg.com/v2-aaeecd605326ba56414edfba4d715794_1440w.jpg">

到这里，AdaMuon已经具备雏形了。我们将算法整理成表：

<img src="https://pica.zhimg.com/v2-68d717b931b02e9d6d68978eff58be94_1440w.jpg">

## 实验

来看看效果。

<img src="https://pic1.zhimg.com/v2-d997ba187ae903a7415a29c35dacc738_1440w.jpg">

<img src="https://pic4.zhimg.com/v2-e2d5e91f454a4dd0655537bdc09e6a63_1440w.jpg">

<img src="https://pic4.zhimg.com/v2-c8fcff8e34f3e9868b0484fc41a92f9d_1440w.jpg">

如果是做预训练的话，我觉得上面三张图已经很能说明问题了。无论是训练效率，还是评测集的跑分，AdaMuon都能取得很好的结果。我现在还在进行AdaMuon scaling law的实验，还有一些其他架构的实验。从目前的初步结果来看，还挺有希望的。

## 一些讨论

我们讨论一些补充性发现。虽然 AdaMuon 在效果上优于 Muon，但不可否认的是，由于需要额外缓存一组二阶动量矩阵，AdaMuon 对memory的需求也相应提高。然而，在消融实验中我们观察到了一个非常有趣的现象：

<img src="https://pic1.zhimg.com/v2-f6820bc2a9df0b0a46eaa8bd4a02c5bc_1440w.jpg">

Muon+Sign（我们暂称其为 SiMuon）的表现显著优于原始 Muon，甚至接近 AdaMuon 的效果。而 SiMuon 相比 Muon 仅增加了一个符号操作，计算开销极小，因此我们认为这是一种极具潜力的 Muon variant。遗憾的是，受限于计算资源与时间，我未能进行大规模实验验证。然而，从现有实验结果已可清晰看出 SiMuon 所具备的潜力。

最后，我们从学术角度探讨几个可能的改进方向。首先，针对如 ResNet 等广泛使用卷积结构的模型，Muon 目前仅适用于二维矩阵参数，而卷积核通常以高维张量形式存在。因此，能否为高维张量设计相应的正交优化器，是一个值得探索的重要问题。

其次，Muon 在分布式训练中的通信开销仍有优化空间。当前实现需要跨数据并行组 gather 各节点的动量分量，这一操作引入了额外的通信负担。能否设计一种通信效率更高的 Muon 变体，或寻求一种既保持其正交更新特性、又具备类似 Adam 那样低通信开销的替代方案，也具有重要的研究价值。

最终，一个更为根本的研究目标是：能否设计出一种真正意义上的矩阵级二阶优化器？此类方法有望在矩阵空间中对曲率进行建模，从而实现对优化方向的更精确调整，从根本上突破现有基于标量或元素级自适应方法的局限。实现这一目标将不仅能够显著提升优化效率，更可能为深度学习的优化理论带来新的突破。

上述三个方向中，解决第一个问题可形成一篇具有扎实学术价值的论文，解决第二个问题将直接推动工业界的大规模应用；而第三个方向——实现真正的矩阵级二阶优化器，我认为堪称优化器领域的一颗明珠。
