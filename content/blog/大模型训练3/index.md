---
title: 大模型训练3：从矩阵乘法看大模型如何切分
summary: 大模型训练
date: 2025-08-08
type: docs
math: true
tags:
  - 大模型
image:
  caption: 
---


<!-- {{< math >}} $$ {{< /math >}} -->

我们来聊聊大模型是怎么“拆分”（spilt or shard）到多个加速器上的。

其实本质就是：大多数 LLM 主要由矩阵乘法组成，所以你只要搞懂“多个设备如何一起做矩阵乘法”，你基本就明白模型是怎么并行训练的了。接下来我们会基于 TPU 上的通信开销，给出一个简单直观的“分布式矩阵乘法”理论，帮助你理解这种跨设备的计算到底是怎么进行的。

## 分区术语和集体操作

当我们在一万个 TPU 上训练一个 LLM 时，其实本质上做的计算和在一个 TPU 上训练是一样的。不同的是——模型太大了，没法全都塞进单个 TPU 的 HBM（高带宽内存），所以我们不得不把数据“切开”。这个切开数组的过程，就叫做 “分片”（sharding）或 “划分”（partitioning）。

举个例子，假设我们有一个二维数组 {{< math >}} $A$ {{< /math >}}，它被平均分成 4 份，分布在 4 个 TPU 上。从“全局”角度看，这个数组还是原来的大小，比如说是 (4, 128)；但从每个设备本地看，每块只有 (2, 64)，也就是说每个 TPU 实际只存了一小部分——总共 1/4 的数据。

<img src="https://pic2.zhimg.com/v2-6aefdd0beacc7518906c3e42b473eac1_1440w.jpg">

接下来我们会把这个想法推广到任意维度的数组，并介绍在多设备之间操作这些分片数组时要用到的一些“集体操作”（collective operations），比如 all-reduce、all-gather 等等。

### 统一的分片表示法

我们用一种 “命名轴” (named-axis notation) 的变体记法来描述张量是如何在多个设备之间以块（block）方式分片（shard）的。具体来说，我们假设设备之间构成了一个二维或三维的网格（device mesh），这个网格的每个维度都有一个名字，比如 {{< math >}} $X$ {{< /math >}}、{{< math >}} $Y$ {{< /math >}} 或 {{< math >}} $Z$ {{< /math >}}。接着，我们可以通过描述数组的哪些维度沿着哪些设备网格的轴进行了划分，来指定这个张量是如何被分布式地存储的。这个映射方式就叫做 Sharding（分片策略）。

我们以之前的图为例，我们有：

- Sharding 表达式： {{< math >}} $A[I_X, J_Y]$ {{< /math >}}。这句话的意思是：数组 {{< math >}} $A$ {{< /math >}} 的第一个维度 {{< math >}} $I$ {{< /math >}} 被分布在设备网格的 {{< math >}} $X$ {{< /math >}} 轴上，第二个维度 {{< math >}} $J$ {{< /math >}} 被分布在 {{< math >}} $Y$ {{< /math >}} 轴上。换句话说，{{< math >}} $I$ {{< /math >}} 维度按 {{< math >}} $X$ {{< /math >}} 分片，{{< math >}} $J$ {{< /math >}} 维度按 {{< math >}} $Y$ {{< /math >}} 分片，那么最终的结果就是，每个设备上只保存整个数组的 {{< math >}} $1/(|X|*|Y|)$ {{< /math >}} 部分。
- 设备网格 Mesh ：设备网格定义如下，这表明我们有 4 个 TPU，排成一个 2×2 的二维网格，其中第一个维度叫 {{< math >}} $X$ {{< /math >}}，第二个维度叫 {{< math >}} $Y$ {{< /math >}} 。

```python
Mesh(devices=((0, 1), (2, 3)), axis_names=('X', 'Y'))
```

- 本地分片大小（local shape）：假设数组 {{< math >}} $A$ {{< /math >}} 的全局维度是 {{< math >}} $(I,J)$ {{< /math >}} ，那么每个设备拿到的本地数据大小就是 {{< math >}} $(I/2, J/2)$ {{< /math >}} 。

总结一下：通过指定 {{< math >}} $A[I_X,J_Y]$ {{< /math >}} 和设备网格的结构，我们就知道每个 TPU 拿到数组 {{< math >}} $A$ {{< /math >}} 的哪一部分、拿多少。这种方式很适合大模型训练中手动设计 sharding 策略，尤其在 TensorFlow XLA、JAX、TPU 等系统中非常常见。你可以想象成是“把数据按指定方向切开、分别塞进每个计算核心”。

我们再给一个2D 张量在单个轴上进行 2D 分片的例子来更加直观的感受一下：{{< math >}} $A[I_{XY},J]$ {{< /math >}} 。这里的意思是：张量 {{< math >}} $A$ {{< /math >}} 的第一维 {{< math >}} $I$ {{< /math >}} 被同时沿着设备网格的 {{< math >}} $X$ {{< /math >}} 和 {{< math >}} $Y$ {{< /math >}}  两个方向进行分片；第二维 {{< math >}} $J$ {{< /math >}}  没有分片，所有设备都完整保留这个维度；这跟前面的 {{< math >}} $A[I_X,J_Y]$ {{< /math >}}  相比，不是同时沿两个张量维度分片，而是把同一个维度 {{< math >}} $I$ {{< /math >}} ，在设备网格的两个方向上切了。

这种分片方式在一些大模型里非常实用，尤其是当我们只想并行某个维度的计算，但又希望利用多个方向的设备拓扑结构（比如在 TPU 的 {{< math >}} $X$ {{< /math >}}  和 {{< math >}} $Y$ {{< /math >}}  轴上都利用上）。

好的，again，加深印象，让我们再来可视化一下这些不同的sharding 是怎么把一个二维数组分配到 4 个设备上的。

<img src="https://pic2.zhimg.com/v2-0798764aca58ec4aa35c64f7f3ef8263_1440w.jpg">

我们可以完全复制（Fully Replicated），写作： {{< math >}} $A[I,J]$ {{< /math >}} （没有任何分片标记），这个张量在每个设备上都有一份完整的副本。

<img src="https://pic4.zhimg.com/v2-c7f0947d0d24a2704c8dcde1d934598d_1440w.jpg">

当我们想表示张量的某个维度已经被 shard到某个 mesh 轴（硬件设备网格的某一维）上时，就会在该维度下面加一个 mesh 轴的下标。比如这个写法： {{< math >}} $A[I_X,J]$ {{< /math >}}，表明 {{< math >}} $I$ {{< /math >}} 被划分到了 mesh 的 {{< math >}} $X$ {{< /math >}} 轴上，而 {{< math >}} $J$ {{< /math >}} 没有被划分。

<img src="https://pica.zhimg.com/v2-32979a20e21b5f40caa15281951c7c78_1440w.jpg">

那么 {{< math >}} $A[I_X,J_Y]$ {{< /math >}} 的示意图也就比较好理解了：

<img src="https://picx.zhimg.com/v2-f67591e28038f188d951854e656e7629_1440w.jpg">

我们也列举了一些其他可能的情况：

<img src="https://pica.zhimg.com/v2-28b75ec591e05611574ecaf2324ae040_1440w.jpg">

这里，{{< math >}} $A[I_{XY},J]$ {{< /math >}} 表示我们把 mesh 轴 {{< math >}} $X$ {{< /math >}} 和 {{< math >}} $Y$ {{< /math >}} 组合成一个更大的一维（flattened）维度，然后将张量的轴 {{< math >}} $I$ {{< /math >}} 按照这个展开后的维度来切分，也就是说：{{< math >}} $I$ {{< /math >}} 轴的数据会被平均分布到 {{< math >}} $X\times Y$ {{< /math >}} 的所有设备上； {{< math >}} $J$ {{< /math >}} 轴的数据没有被划分，会在每个设备上完整保留。

最后要注意的是：同一个 mesh 轴不能同时用于切分多个张量维度。比如这个写法：{{< math >}} $A[I_X,J_X]$ {{< /math >}}  就是不合法的，因为你试图把轴 {{< math >}} $I$ {{< /math >}} 和 {{< math >}} $J$ {{< /math >}} 都沿着 mesh 的 {{< math >}} $X$ {{< /math >}} 维度切分。直白说就是：一条 mesh 轴（比如 {{< math >}} $X$ {{< /math >}} ）在某个张量维度上用过之后，就相当于“用完”了，不能再用来切别的维度了。这是因为每个 mesh 维度只能映射到一个逻辑维度的切分上，否则就会发生冲突 —— 同一个设备就不知道该接收哪个维度的数据了。这在实际的模型并行中会导致资源分配混乱或者通信无法对齐。

## 分布式数组的计算

当你的数据被切分在多个设备上，你还想对它做数学运算时，一个核心问题是：在切分数据和并行计算的前提下，额外代价来自哪里？答案其实取决于你要做的操作类型：

- 元素级运算（elementwise ops）：比如加法、ReLU、逐元素乘法……这些操作每个元素自己算自己的，不需要跨设备交流。所以基本没代价，在哪个设备上的元素就在哪算，完事儿。
- 涉及跨元素/跨设备的运算（比如矩阵乘法）：这就复杂了。但好消息是：深度学习的大部分重计算，其实就是矩阵乘法。而矩阵乘法的通信代价是可以系统分析的，我们接下来就只关注它。

我们现在正式进入重点：如何对「被切分」的矩阵进行乘法？

你可以把整个过程理解成：把矩阵的碎片搬来搬去，然后在每台设备上单独做乘法，最后把结果拼起来。这背后的关键问题是：怎么切分矩阵，决定了你需不需要通信（也就是设备之间要不要发数据）。比如：

{{< math >}} $$A[I_x, J]  \cdot B[J, K_y] \rightarrow C[I_x, K_y]$$ {{< /math >}}

这时候：{{< math >}} $A$ {{< /math >}} 的第一个维度 {{< math >}} $I$ {{< /math >}} 被切在了 {{< math >}} $X$ {{< /math >}}  轴，{{< math >}} $B$ {{< /math >}} 的第三个维度 {{< math >}} $K$ {{< /math >}}  被切在了 {{< math >}} $Y$ {{< /math >}}  轴；{{< math >}} $J$ {{< /math >}}  没被切，是“完整的”；最后的输出 {{< math >}} $C$ {{< /math >}}  也被切分了。不过好消息是， {{< math >}} $J$ {{< /math >}}  是乘法中需要 sum 的那一维（contracting dim），没被切开，就不用通信。也就是说，每台设备本地拿到的数据就能完成计算，不用跟别人说话，效率高。

但如果你想让输出  {{< math >}} $C$ {{< /math >}}  是完整的、不切的，比如：

{{< math >}} $$A[I_x, J] \cdot B[J, K_y] \rightarrow C[I, K]$$ {{< /math >}}

那问题就来了，你得让每台设备把结果发出去，或者让别人把 {{< math >}} $A$ {{< /math >}}  发进来，反正得有通信。而且你有两种选择：拷贝 {{< math >}} $A$ {{< /math >}} ，或者拷贝 {{< math >}} $C$ {{< /math >}} ，哪种划算得算一下。现在我们把所有可能的切法总结成 4 种情况，每种都有对应的通信策略：

- 没有任何输入在 sum 的维度上被切；
- 有一个输入在 sum 的维度上被切；
- 两个输入都在 sum 维度上被切；
- 两个输入都在“非 sum 的维度”上按同一个轴被切。

好，已经绕晕了，可能看半天也不知道啥意思。没关系，我们接下来一个一个来看。

在开始之前，建议带着矩阵分块乘法的视角。

### Case 1：两个矩阵的输入都没有在「被求和的维度」上做切分

这其实是最轻松的一种情况。我们先看个定理（放心，不难）：

当你在做矩阵乘法时，只要「1. 被求和的维度没被切开」，并且「2. 两个矩阵的输入在非求和维度上不切在同一轴」，那计算就能顺利进行，而且输出的切分规则也会自动匹配输入的切法。

有点晦涩绕口，举个例子：{{< math >}} $A[I_x, J] \cdot B[J, K_y] \rightarrow C[I_x, K_y]$ {{< /math >}} 

在这个例子里：

1. {{< math >}} $J$ {{< /math >}} 是求和的维度；{{< math >}} $J$ {{< /math >}}没有被切开；
2. {{< math >}} $I$ {{< /math >}} 和 {{< math >}} $K$ {{< /math >}} 不是求和维度，并且没有在同一个mesh 轴上切分。

所以每台设备都可以拿到自己那一块的 {{< math >}} $I$ {{< /math >}} 和 {{< math >}} $K$ {{< /math >}} 的数据， 然后在本地用完整的 {{< math >}} $J$ {{< /math >}} 做乘法和 sum。这完全不需要通信，因为直接在各自的设备上就可以把结果算出来，输出自动是按照 {{< math >}} $X$ {{< /math >}} 和 {{< math >}} $Y$ {{< /math >}} 轴切分的。这也是为什么这种情况被称为最理想的情况：没有通信，只有计算，本地就能搞定一切。

一句话总结：如果求和的维度没被切，乘法就能本地算完，不用通信，省时省力还高效。

### Case 2：只有一个矩阵的「被求和的维度」被切分了

假设我们要做一个矩阵乘法： {{< math >}} $A[I, J_x] \cdot B[J, K] \rightarrow C[I, K]$ {{< /math >}}。也就是说，矩阵 {{< math >}} $A$ {{< /math >}} 在 {{< math >}} $J$ {{< /math >}} 这个被求和的维度上被按 {{< math >}} $X$ {{< /math >}} 轴切分了，而 {{< math >}} $B$ {{< /math >}} 是完整复制的。这种情况下，我们不能直接让每个设备拿着自己本地的 {{< math >}} $A$ {{< /math >}} 和 {{< math >}} $B$ {{< /math >}} 就地做乘法，因为 {{< math >}} $A$ {{< /math >}} 的每台设备只持有 {{< math >}} $J$ {{< /math >}} 的一部分，而我们要求的是把 {{< math >}} $J$ {{< /math >}} 全部都 sum 一遍。如果你少了 {{< math >}} $J$ {{< /math >}} 的一部分，那这个 sum 出来的结果就不对。

为了解决这个问题，我们要做的是“先 AllGather 再乘”：{{< math >}} ${\rm AllGather}_x(A[I, J_x]) \rightarrow A[I, J]$ {{< /math >}} ，{{< math >}} $ A[I, J] \cdot B[J, K] \rightarrow C[I, K] $ {{< /math >}} 。也就是说：我们把 {{< math >}} $A$ {{< /math >}} 沿着切分的维度上的碎片 gather 回来，每个设备都拿到 {{< math >}} $A$ {{< /math >}} 的完整副本，然后再和 {{< math >}} $B$ {{< /math >}} 做乘法。AllGather 的作用就是“把沿某个轴切分的数据，重新收集成完整的副本”。当然，我们也可以只 gather 掉一部分，比如：{{< math >}} $A[I_{xy}, J] \rightarrow A[I_{y}, J]$ {{< /math >}}。

注意，AllGather 也可以用于非求和维度，比如我们本来有：{{< math >}} $A[I_x, J] \cdot B[J, K] \rightarrow C[I, K]$ {{< /math >}}。虽然 {{< math >}} $J$ {{< /math >}} 没被切（很好），但 {{< math >}} $I$ {{< /math >}}  被切了，我们如果想要最后的输出 {{< math >}} $C$ {{< /math >}}  是不切分的，也可以：{{< math >}} ${\rm AllGather}_x(C[I_x, K]) → C[I, K]$ {{< /math >}} 。但这时候我们有选择权，可以在乘法前或乘法后再 gather，没强制。

这里还有几个小问题：

1. AllGather 是怎么被执行的？

我们前面说了，AllGather 的目的是把每台设备手里那一小块数据互相传一传，最后每台设备都拥有完整的数据。那这个“互相传一传”到底怎么做的？

想象你有一圈设备，每台都有一小份数据。比如有 8 台设备，每台有数组的 1/8，我们的目标是让每一台设备最终拥有 shard 0~7 的全量副本。那么我们有两种方式：

- 单向传输：每台设备只往一个方向（比如顺时针）传：D0 → D1 → D2 → … → D7 → D0（回到起点），每轮，每台设备把新收到的 shard 再往下传，总共需要 N - 1 轮（因为第一个 shard 你自己已经有了）；每一跳传的数据大小是： {{< math >}} ${\rm total \ size}/N$ {{< /math >}} 。
- 双向传输：每台设备同时往前后两个方向传，也就是D0 同时传给 D1 和 D7，D1 同时传给 D2 和 D0，以此类推。这样做传输次数可以减半：最多只需要 {{< math >}} ${\rm ceil}(N/2)$ {{< /math >}}  跳。但每一跳要传两个 shard 的大小：{{< math >}} $2*{\rm total \ size} / N$ {{< /math >}} 。

<img src="https://pic1.zhimg.com/v2-1cf9e884d543715f3a29a36ded15d7f8_b.webp">

2. AllGather 要花多久时间？

我们现在来算一下用“双向传输”的 AllGather 大概要花多久。假设：你的总数组大小是 {{< math >}} $V$ {{< /math >}}  字节，沿着某个维度被分成了 {{< math >}} $|X|$ {{< /math >}}  份（即有 {{< math >}} $|X|$ {{< /math >}}  台设备分着这个数据），每一台设备之间的通信带宽是 {{< math >}} $W_{ICI}$ {{< /math >}}  。每一跳会向两个方向传 {{< math >}} $V/|X|$ {{< /math >}}  个字节，所以每一跳花的时间是  {{< math >}} $T = \frac{2\cdot V}{|X|\cdot W_{ICI}}$ {{< /math >}} 。由于我们是双向传输，所以只需要跳 {{< math >}} $|X| / 2 $ {{< /math >}}  次就能传遍所有设备，因此，总时间为：

{{< math >}}  $$T_{total} = \frac{2\cdot V \cdot |X|}{2\cdot |X| \cdot W_{ICI}} = \frac{V}{W_{ICI}}$$ {{< /math >}}

是的，你没看错。这个公式里根本没有设备数量 {{< math >}} $|X|$ {{< /math >}} 。这意味着，不管你 AllGather 是 4 台、8 台、32 台——只要链路带宽一样、数据量一样，总花费时间是一样的。虽然设备越多跳得多，但每次跳的数据就变少了，两者正好抵消。

总结一句话： 在带宽主导的场景下，AllGather（或 AllReduce、ReduceScatter）真正决定速度的，是你传了多少数据和你那条线的带宽，不是设备数量。

3. 关于 ICI 延迟的一些注意点

虽然之前我们说 AllGather 的时间跟设备数量无关，但有个例外：当数据量特别小的时候。这是因为每次跳都有一个固定的延迟开销，大概是 1 微秒，不管你传多少字节。如果每次跳都只传一点点数据，那每次传输用的时间其实不是带宽限制，而是被这个 1 微秒的固定开销给主导了。这种情况我们就叫：Latency-bound（延迟主导） 而不是 Throughput-bound（带宽主导）。也就是说，数据太小的时候，哪怕你带宽很快，还是慢！因为你光是“发起一次跳”的时间都比传输时间还长了。

我们可以在 TPU v5e 的一个 8×16 的 slice 上做一个实验：把数组沿着 16 的维度切开，然后做 AllGather，正好是个完整的双向环。下面是结果图：

<img src="https://picx.zhimg.com/v2-c3b604b09930e3a2d0645ef8eea65495_1440w.jpg">

- 橙色线：实际 AllGather 成功的带宽（多少数据每秒被收集起来）
- 蓝色线：推测的单向链路带宽（根据硬件模型反推出来的）

结论很直白：实际能用到的带宽，大概只有理论峰值（4.5e10）的 95%

这个带宽只有当数组大小到达 10MB 左右时才能稳定发挥，10MB/16 = 每台设备大概 500KB 数据的时候，性能才最稳。

一句话总结：当数据特别小（比如每台设备不到 500KB）的时候，AllGather 会受到 延迟 而不是 带宽 的限制，导致变慢。所以，如果你做的是超小 batch 的操作，记得别指望通信能快。

### Case 3：两个矩阵都在「被求和的维度」上做了切分

设我们要计算：{{< math >}} $A[I, J_x] \cdot B[J_x, K] \rightarrow C[I, K]$ {{< /math >}}，这意味着：  {{< math >}} $A$ {{< /math >}} 和  {{< math >}} $B$ {{< /math >}}  都在  {{< math >}} $J$ {{< /math >}}  维做了分片。计算本地矩阵乘法当然可以做（因为每台设备都有  {{< math >}} $J_x$ {{< /math >}}  的局部部分），但每个设备只能算出“部分和”（partial sum），它看不到完整的 {{< math >}} $J$ {{< /math >}}  ，所以它算的乘法结果也是不完整的。我们把这种不完整的结果写作：

{{< math >}} $$A[I, J_x] \cdot {\rm _{Local}}B[J_x, K] \rightarrow C[I, K]\{U_x\}$$ {{< /math >}}

这里的  {{< math >}} $\{U_x\}$ {{< /math >}} 表示结果还“没求和完”（unreduced），要进一步合并。那怎么让这些 partial sum 合起来？这就牵扯到一个操作，叫 AllReduce。AllReduce 会让所有设备把自己算出来的部分结果发给邻居，然后大家互相累加，最后所有设备都得到了完整的和（full sum）： {{< math >}} ${\rm AllReduce}_X C[I, K]\{U_x\}  \rightarrow C[I, K]$ {{< /math >}} 。

同样的，我们也有关于 AllReduce 的几个问题。

1. AllReduce 到底有多“贵”？

想象一下你现在有好多台 TPU，每台上都有一个部分结果（shard），我们现在想要每台设备都拿到这个结果的总和。那么每台设备要把自己的结果发给别的设备，同时接收别人的，然后自己把所有这些结果加在一起，每台设备最后都有一个完整的总和。这个过程比 AllGather 要贵，因为在 AllGather 中，每台设备只需要收集别人的一小部分数据。但在 AllReduce 里，每个 shard 本身就是一个“全量大小”的数组（跟最终结果一样大），所以传输的数据量是 AllGather 的两倍左右。

我们也可以把 AllReduce 拆成两个步骤理解：1. ReduceScatter：先把每台设备上的“部分和”加起来，然后把结果按设备数切片，分发出去（每台设备只保留其中一块）；2. AllGather：再把这些分散的部分收集回来，让每台设备都拿到完整结果。这两个操作合起来，就等于 AllReduce。所以可以理解为：AllReduce ≈ 2 × AllGather 的开销。

2. 关于ReduceScatter

回顾一下 AllReduce 做了两件事：把所有设备的部分结果加起来，再对总的结果切分，分发到每台设备；聚合所有设备的结果。而 ReduceScatter 只做了第一步 —— 加完之后，每台设备只保留自己那一部分，而不是拿到全部数据。换句话说，原本你有一堆“没加完的结果”，ReduceScatter 负责把它们加完，然后再把最终结果按设备分开发下去（scatter）。所以最后的数组不再是部分和状态，而是已经按设备分好了的结果。这么说还是有点抽象，我给一个例子。假设我们有一个乘法

```markdown
# A: [I, J], B: [J, K]
C = A · B
```

ReduceScatter 的做法是：

```markdown
ReduceScatter (比如我们选择 K 维 sharding)：
- Device 0 最后保留 C[:, 0:K/4]
- Device 1 最后保留 C[:, K/4:K/2]
- Device 2 最后保留 C[:, K/2:3K/4]
- Device 3 最后保留 C[:, 3K/4:K]
```

而并非每台设备上原有的结果：

```markdown
Device 0: C0 = A0 · B0，[I, K]
Device 1: C1 = A1 · B1
Device 2: C2 = A2 · B2
Device 3: C3 = A3 · B3
```

你可以理解成：我们一边把结果加起来（Reduce），一边按 {{< math >}} $K$ {{< /math >}} 切块丢到不同设备（Scatter）。 另外，ReduceScatter 跟 AllGather 很像，其实 ReduceScatter 的通信方式也跟 AllGather 很像，区别在于：AllGather 是每台设备保留每一块；ReduceScatter 是每台设备把东西加一加，然后只留自己那一块。所以两者的延迟（latency）也差不多，除了加法这一步要多花点时间。

<img src="https://pic3.zhimg.com/v2-734b70432d6c0c3d89e4bfabe07bd152_b.webp">

### Case 4：两个矩阵的「非被求和的维度」被沿着同一个 mesh 轴切分了

在实际情况中，会有一种“你不能这么干”的情况，比如这样：

{{< math >}} $$A[I_X, J] \cdot B[J, K_X] \rightarrow C[I_X, K_X]$$ {{< /math >}}

这为什么不行？

想象下，有一组设备都沿着 {{< math >}} $X$ {{< /math >}} 轴做了切分。那每个设备 {{< math >}} $i$ {{< /math >}} 就只拿到结果矩阵 {{< math >}} $C$ {{< /math >}} 中第 {{< math >}} $(i,i)$ {{< /math >}} 个对角线位置的 shard。换句话说，每个设备只知道自己那一点的信息，完全不知道别的设备上算出来的部分。所以整个矩阵只知道对角线，其他值全是未知的。 这就没法还原完整的结果，所以这个切法是非法的。

为了解决这个问题，我们可以先 AllGather 一些维度。具体来说，我们有两种选择：

1. AllGather A： {{< math >}} ${\rm AllGather}_X A[I_X, J] \rightarrow A[I, J],\ \ A[I, J] · B[J, K_X] \rightarrow C[I, K_X]$ {{< /math >}}
2. AllGather B：  {{< math >}} ${\rm AllGather}_X B[J, K_X] \rightarrow B[J, K], \ \ A[I_X, J] · B[J, K] \rightarrow C[I_X, K]$ {{< /math >}}

也就是说，我们必须让 {{< math >}} $X$ {{< /math >}} 只出现一次，不能同时出现在 {{< math >}} $A$ {{< /math >}} 和 {{< math >}} $B$ {{< /math >}} 的非收缩维上。哪个维度 AllGather，要看你后面想怎么继续切。

## 更深入地理解 TPU 的通信原语

我们在前面四种矩阵乘法的分片（sharding）场景中，其实已经接触了几种非常关键的通信操作：

1. AllGather：合并分片，用来取消某个维度的切分，把各个设备手上的分片合成完整数据。
2. ReduceScatter：求和并重新分片，用来“去掉未归约的后缀”（比如  {{< math >}} $\{U_X\}$ {{< /math >}} ），但并不会把数据变成完全复制的，而是重新分片到另一个维度上。
3. AllReduce：全量求和 + 广播，它最后会让每个设备上都拥有完整的求和结果。换句话说，ReduceScatter 是 “算完再分”，AllReduce 是 “算完每人都留一份”。

然而，还有最后一个通信原语——AllToAll，这个在我们之前讨论的几种矩阵乘法中还没出现，但它在 MoE（专家混合模型） 和某些其他需要复杂路由的计算中非常常见。

### AllToAll

简单理解，AllToAll 就是一种跨设备的“转置”或“重新分片”操作。它的本质是在多个设备之间，把数据从一个维度的切分方式，重新映射到另一个维度。比如原来是按 batch 切的，现在要按 expert 分，那就需要 AllToAll 来调整这些 shard 的布局。

举个例子，假设你有 8 个 TPU 芯片，原来是每个芯片拿一块 batch 的数据（比如按  {{< math >}} $I$ {{< /math >}} 维切片），现在你要让每个芯片专门负责一部分 expert（比如按  {{< math >}} $J$ {{< /math >}} 维切片），那就得把  {{< math >}} $I$ {{< /math >}} 维的分片“转”到  {{< math >}} $J$ {{< /math >}} 维，这时候就得用 AllToAll。你可以把 AllToAll 想象成把张量的一个下标从一个维度“搬”到另一个维度。

AllToAll 和 AllGather 最大的不同是：

- AllGather 是把每个分片的数据 复制到所有设备，每个设备最后都会拿到全量；
- AllToAll 是互相交换分片的一部分，但每个设备只拿自己需要的那块数据，不做全量复制。

正因为 AllToAll 不用复制所有 shard 的内容，所以它的通信成本其实比 AllGather 要低不少——通常 AllToAll 的通信开销是 AllGather 的 四分之一。这在需要频繁 reshuffle 数据布局的模型中（比如 MoE）能省下不少代价。

<img src="https://picx.zhimg.com/v2-0de83734c733ad136306ed16315c3989_b.webp">

### 更深入聊聊 ReduceScatter

虽然听起来它只是众多通信操作之一，但其实 ReduceScatter 是 AllGather 的“反操作”，两者是一对好搭档。举个简单例子：正向传播时我们用 AllGather 把各个设备的分片拼成完整张量；反向传播时我们需要把梯度重新按分片打散，那就用 ReduceScatter 把这些结果 “散回去”。你可以把它想象成：“AllGather 是拼图，ReduceScatter 是拆图。”

我们用公式来表示一下， {{< math >}} ${\rm AllGather}_X A[I_X] \rightarrow A[I]$ {{< /math >}} ，意思是原来 {{< math >}} $A$ {{< /math >}} 被按 {{< math >}} $I$ {{< /math >}} 维在 {{< math >}} $X$ {{< /math >}} 轴上分片了，现在我们 AllGather 一下，把它拼成完整的 {{< math >}} $A[I]$ {{< /math >}} 。反过来，{{< math >}} ${\rm ReduceScatter}_X A’[I]\{U_X\} \rightarrow A’[I_X]$ {{< /math >}}，就是我们把反向的梯度  {{< math >}} $A'[I]$ {{< /math >}} 拆成分片状态。

那么为什么我们需要ReduceScatter？这是因为有时候我们其实不想立刻 AllGather 把整个张量都还原，因为那太耗资源了。比如下面这种情况：

{{< math >}} $$A[I, J_X] \cdot B[J_X, K] \rightarrow C[I, K_X]$$ {{< /math >}}

两个输入在 {{< math >}} $J$ {{< /math >}} 维度上都分片了，我们本来是要 AllReduce 来合并结果。但有时候可以：

1. 先 LOCAL 做乘法；
2. 再 ReduceScatter 把结果拆成分片状态；
3. 如果以后需要再 AllGather，再做也不迟！

```markdown
Step 1: LOCAL 乘法（每个设备做自己那份）：
Device 0: C0 = A0 · B0
Device 1: C1 = A1 · B1
Device 2: C2 = A2 · B2
Device 3: C3 = A3 · B3

Step 2: ReduceScatter (比如我们选择 K 维 sharding)：
- Device 0 最后保留 C[:, 0:K/4]
- Device 1 最后保留 C[:, K/4:K/2]
- ...
```

这样你就可以一直保持 tensor 是分片的状态，不用每一步都还原成全量，更省显存，更高效。

```markdown
# 假设 A 和 B 是按 J 分片的矩阵
# 原始方式（AllReduce）：
C_partial = local_matmul(A_local, B_local)
C = all_reduce(C_partial)  # 得到全量的 C

# 更高效的方式（ReduceScatter）：
C_partial = local_matmul(A_local, B_local)
C_sharded = reduce_scatter(C_partial, axis='K')  # 直接输出分片结果
```

总结一句话：ReduceScatter 不仅是 AllGather 的逆操作，它还能让你控制结果接下来如何被分片，是大规模并行计算里的关键角色。

## 经典总结

好了，晦涩的文字到此结束，总结一下：

1. 什么是 Sharding？

一个数组的分片方式由两个部分决定：

- Mesh：定义了 TPU 硬件的物理轴（比如 {{< math >}} $X$ {{< /math >}}、{{< math >}} $Y$ {{< /math >}}），比如 Mesh({'X': 4, 'Y': 8}) 表示 4x8 的设备网格。
- Sharding：指定了数组的哪些逻辑维度要分布到哪些 Mesh 轴上。

例如，{{< math >}} $A[I_{XY},J]$ {{< /math >}} 表示 {{< math >}} $A$ {{< /math >}} 的第一个维度被按 {{< math >}} $X$ {{< /math >}} 和 {{< math >}} $Y$ {{< /math >}} 轴一起分成 32 份。

2. 带分片的数组怎么做计算？

跟没分片时类似，但如果要对被分片的维度做矩阵乘法，就需要通信操作。我们可以把情况分成以下 4 类：

- 没有分片在被求和维度上： 不需要任何通信操作。
- 只有一个矩阵在求和维度上分片： 用 AllGather 把某个输入先聚合起来再做计算。
- 两个矩阵都在求和维度上分片：每个设备先做本地计算，再用 AllReduce 或 ReduceScatter 聚合结果。
- 两个矩阵在非求和维度上用了相同的 mesh 轴做分片：不合法，必须 AllGather 其中一个数组。

3. TPU 上的 4 个核心通信操作

TPU 中有 4 个基础通信原语（primitive）来支持分布式矩阵乘法：

- AllGather：取消某个维度上的分片，把数据全 gather 到每个设备;
- ReduceScatter：对某个维度上的分片做加和，然后再进行分片;
- AllReduce：先 ReduceScatter 再 AllGather（其实是组合操作）;
- AllToAll：改变分片的布局，即把分片从一个维度换到另一个。

这一章写起来是真不容易，写完后才发现已经是万字长文了。
