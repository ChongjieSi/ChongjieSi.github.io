<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="LoRA-Dash">
  <meta property="og:title" content="LoRA-Dash"/>
  <meta property="og:description" content="LoRA-Dash"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="twitter:image" content="/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="lora-dash, peft">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LoRA-Dash</title>
  <link rel="icon" type="image/x-icon" href="/images/2024-lora-dash/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Correct CSS paths -->
  <link rel="stylesheet" href="/css/project/bulma.min.css">
  <link rel="stylesheet" href="/css/project/bulma-carousel.min.css">
  <link rel="stylesheet" href="/css/project/bulma-slider.min.css">
  <link rel="stylesheet" href="/css/project/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/css/project/project.css">

  <!-- Correct JS paths -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="/js/project/fontawesome.all.min.js"></script>
  <script src="/js/project/bulma-carousel.min.js"></script>
  <script src="/js/project/bulma-slider.min.js"></script>
  <script src="/js/project/project.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><br>Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="../../../" target="_blank">Chongjie Si</a><sup>*1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=izH84aAAAAAJ" target="_blank">Zhiyi Shi</a><sup>*2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=UtzVbh0AAAAJ" target="_blank">Shifan Zhang</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="https://english.seiee.sjtu.edu.cn/english/detail/842_802.htm"target="_blank">Xiaokang Yang</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <a href="https://vcg.seas.harvard.edu/people" target="_blank">Hanspeter Pfister</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="https://shenwei1231.github.io" target="_blank">Wei Shen</a><sup>1</sup>
                  </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University<br><sup>2</sup>Harvard University
                      <!-- <br>Conference name and year -->
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                       <!-- PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/2409.01035v1" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->
                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->
                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2409.01035v1" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Chongjie-Si/Subspace-Tuning" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
          <div class="columns is-centered has-text-centered">
            <img src="/images/2024-lora-dash/loradash.png" width="100%">
          </div>
      <br>
      <br>
      <h2 class="subtitle has-text-centered">
        DoRA consistently outperforms LoRA on the LLaMA family models for commonsense reasoning tasks.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser image -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted height="100%">
        <source src="/images/2024-lora-dash/1.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        A simple illustration for LoRA-Dash.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directionsâ€”critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of task-specific directions during the fine-tuning process, thereby enhancing model performance on targeted tasks. Extensive experiments have conclusively demonstrated the effectiveness of LoRA-Dash, and in-depth analyses further reveal the underlying mechanisms of LoRA-Dash.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- LoRA-Dash-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3"><br>LoRA-Dash</h2>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
           <img src="/images/2024-lora-dash/loradash.png" alt="Method Overview">
           <br>
           <br>
           <div class="columns is-centered has-text-centered">
             <p>
              LoRA-Dash comprises two phases: the pre-launch phase for identifying TSD, 
              and the dash phase to maximize their potential. </p>
           </div>
          </div>
        </div>
      </div>
    </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3"><br>Results</h2>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <h3 class="title is-4">Fine-tuning LLaMA-7B, LLaMA2-7B and LLaMA3-8B for the Commonsense Reasoning Tasks<br>Compared with LoRA</h3>
               <!-- Illumination Row -->
               <div class="columns is-centered">
                <img src="/images/2024-lora-dash/result1.png" width="80%">
               </div>
          <br>
          <br>
          <h3 class="title is-4">Fine-tuning LLaMA-7B, LLaMA2-7B and LLaMA3-8B for the Commonsense Reasoning Tasks<br>Compared with Other Methods</h3>
              <!-- Illumination Row -->
              <div class="columns is-centered">
                <img src="/images/2024-lora-dash/result2.png" width="80%">
              </div>
          <br>
          <br>       
          <h3 class="title is-4">Fine-tuning DeBERTaV3-base and DeBERTaV3-large for the Natural Language Understanding Tasks<br>Compared with LoRA</h3>
              <!-- Illumination Row -->
              <div class="columns is-centered">
                <img src="/images/2024-lora-dash/result3.png" width="80%">
              </div>
          <br>
          <br>
          <h3 class="title is-4">Fine-tuning DeBERTaV3-base for the Natural Language Understanding Tasks<br>Compared with Other Methods</h3>
              <!-- Illumination Row -->
              <div class="columns is-centered">
                <img src="/images/2024-lora-dash/result4.png" width="80%">
              </div>
          <br>
          <br>
          <h3 class="title is-4">Fine-tuning SDXL for the Subject-driven Tasks</h3>
              <!-- Illumination Row -->
              <div class="columns is-centered">
                <img src="/images/2024-lora-dash/sd.png" width="80%">
              </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{si2024unleashing,
        title={Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning},
        author={Si, Chongjie and Shi, Zhiyi and Zhang, Shifan and Yang, Xiaokang and Pfister, Hanspeter and Shen, Wei},
        journal={arXiv preprint arXiv:2409.01035},
        year={2024}
      }</code></pre>
    </div>
</section>

<!--End BibTex citation -->
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. 
            You are free to borrow the of this website, we just ask that you link back to this page in the footer.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->
  </body>
  </html>
